{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6c727f93-2bbf-4481-bf5d-f4df7804582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyquaternion import Quaternion\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils.data_classes import RadarPointCloud, Box\n",
    "from nuscenes.utils.geometry_utils import points_in_box, view_points\n",
    "from nuscenes.scripts.export_2d_annotations_as_json import post_process_coords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "from shapely.geometry import box as shapely_box, MultiPoint\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a6284bf1-01d6-47ca-949a-1395ac8a5f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "# Get the number of available CPU cores\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print(num_cores)\n",
    "\n",
    "# Set the environment variable to use all CPU cores\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(num_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d9d7de-10ec-4f76-8ad0-a9badc2fdf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the replacements for specific categories\n",
    "category_replacements = {\n",
    "    'human.pedestrian.construction_worker': 'pedestrian',\n",
    "    'human.pedestrian.adult': 'pedestrian',\n",
    "    'human.pedestrian.stroller': 'pedestrian',\n",
    "    'human.pedestrian.police_officer': 'pedestrian',\n",
    "    'human.pedestrian.personal_mobility': 'pedestrian',\n",
    "    'human.pedestrian.wheelchair': 'pedestrian',\n",
    "    'vehicle.bus.bendy': 'bus',\n",
    "    'human.pedestrian.child': 'pedestrian',\n",
    "    'vehicle.truck': 'truck',\n",
    "    'vehicle.car': 'car',\n",
    "    'vehicle.motorcycle': 'motorcycle',\n",
    "    'vehicle.trailer': 'trailer',\n",
    "    'vehicle.bicycle': 'bicycle',\n",
    "    'movable_object.barrier': 'barrier',\n",
    "    'vehicle.bus.rigid': 'bus',\n",
    "    'vehicle.emergency.police': 'car'\n",
    "}\n",
    "\n",
    "# Initialize nuScenes dataset\n",
    "nusc = NuScenes(version='v1.0-trainval', dataroot='', verbose=True)\n",
    "\n",
    "def replace_categories(dataset, replacements):\n",
    "    \"\"\"\n",
    "    Replace specified categories with new categories in the dataset.\n",
    "    \"\"\"\n",
    "    for data in dataset:\n",
    "        for annotation in data['annotations']:\n",
    "            if annotation['category_name'] in replacements:\n",
    "                annotation['category_name'] = replacements[annotation['category_name']]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        img_bytes = img_file.read()\n",
    "        encoded_image = base64.b64encode(img_bytes).decode('utf-8')\n",
    "    return encoded_image\n",
    "\n",
    "def convert_3d_to_2d_bbox(box: Box, camera_intrinsic, pose_rec, calibrated_sensor):\n",
    "    \"\"\"\n",
    "    Convert a 3D bounding box to a 2D bounding box.\n",
    "    :param box: The 3D bounding box object.\n",
    "    :param camera_intrinsic: The camera intrinsic matrix.\n",
    "    :return: List of 2D coordinates representing the bounding box.\n",
    "    \"\"\"\n",
    "    # Translate and rotate the box to the ego-pose frame.\n",
    "    box.translate(-np.array(pose_rec['translation']))\n",
    "    box.rotate(Quaternion(pose_rec['rotation']).inverse)\n",
    "\n",
    "    # Translate and rotate the box to the calibrated sensor frame.\n",
    "    box.translate(-np.array(calibrated_sensor['translation']))\n",
    "    box.rotate(Quaternion(calibrated_sensor['rotation']).inverse)\n",
    "\n",
    "    # Filter out corners not in front of the calibrated sensor.\n",
    "    corners_3d = box.corners()\n",
    "    in_front = np.argwhere(corners_3d[2, :] > 0).flatten()\n",
    "    corners_3d = corners_3d[:, in_front]\n",
    "\n",
    "    # Project 3D box to 2D.\n",
    "    corner_coords = view_points(corners_3d, camera_intrinsic, True).T[:, :2].tolist()\n",
    "\n",
    "    # Keep only corners that fall within the image.\n",
    "    final_coords = post_process_coords(corner_coords, (1600, 900))\n",
    "\n",
    "    return final_coords\n",
    "\n",
    "def create_custom_dataset_with_sequences(nusc, C, R, sequence_length=3):\n",
    "    dataset = []\n",
    "\n",
    "    remove_categories = [\n",
    "        'static_object.bicycle_rack', 'vehicle.emergency.ambulance',\n",
    "        'movable_object.trafficcone', 'movable_object.debris',\n",
    "        'movable_object.pushable_pullable', 'vehicle.construction', 'animal'\n",
    "    ]\n",
    "\n",
    "    for scene_idx, scene in enumerate(nusc.scene):\n",
    "        if scene_idx >= 200:\n",
    "            break\n",
    "\n",
    "        sample_tokens = nusc.field2token('sample', 'scene_token', scene['token'])\n",
    "        \n",
    "        for sample_token in sample_tokens:\n",
    "            sample = nusc.get('sample', sample_token)\n",
    "            camera_data = nusc.get('sample_data', sample['data'][C])\n",
    "            calibrated_sensor = nusc.get('calibrated_sensor', camera_data['calibrated_sensor_token'])\n",
    "            pose_rec = nusc.get('ego_pose', camera_data['ego_pose_token'])\n",
    "            camera_intrinsic = np.array(calibrated_sensor['camera_intrinsic'])  # Get the camera intrinsic matrix\n",
    "            image_path = camera_data['filename']\n",
    "            image_data = image_to_base64(image_path)\n",
    "\n",
    "            annotations = []  # Collect annotations for each image\n",
    "            for ann_token in sample['anns']:\n",
    "                ann_record = nusc.get('sample_annotation', ann_token)\n",
    "                if ann_record['category_name'] not in remove_categories:  # Filtering unwanted categories\n",
    "                    box = Box(ann_record['translation'], ann_record['size'], Quaternion(ann_record['rotation']))\n",
    "                    bbox_2d = convert_3d_to_2d_bbox(box, camera_intrinsic, pose_rec, calibrated_sensor)\n",
    "                    if bbox_2d:\n",
    "                        radar_token = sample['data'][R]\n",
    "                        data_path, Tboxes, camera_intrinsic1 = nusc.get_sample_data(radar_token, selected_anntokens=[ann_token])\n",
    "                        pc = RadarPointCloud.from_file(data_path, dynprop_states=[0, 1, 2, 3, 5, 6, 7])\n",
    "\n",
    "                        for radar_box in Tboxes:\n",
    "                            mask = points_in_box(radar_box, pc.points[:3])\n",
    "\n",
    "                            if np.sum(mask) > 0:\n",
    "                                position = pc.points[:2, mask]\n",
    "                                velocity = pc.points[8:10, mask]\n",
    "                                distances = np.linalg.norm(position, axis=0)\n",
    "                                speeds = np.linalg.norm(velocity, axis=0)\n",
    "                                mean_distance = np.mean(distances)\n",
    "                                mean_speed = np.mean(speeds) * 3.6  # Convert m/s to km/h\n",
    "\n",
    "                                if mean_speed and mean_distance and bbox_2d:\n",
    "                                    annotations.append({\n",
    "                                        'category_name': ann_record['category_name'],\n",
    "                                        'bb_size': bbox_2d,\n",
    "                                        'distance(m)': mean_distance,\n",
    "                                        'speed(km/hr)': mean_speed\n",
    "                                    })\n",
    "\n",
    "                if annotations:\n",
    "                    dataset.append({\n",
    "                        'image_data': image_path,\n",
    "                        'annotations': annotations,\n",
    "                    })\n",
    "\n",
    "    dataset = replace_categories(dataset, category_replacements)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a75dc1-b39c-4a74-b59c-0591251d975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your custom dataset\n",
    "thesis_dataset1 = create_custom_dataset_with_sequences(nusc,'CAM_FRONT','RADAR_FRONT')\n",
    "print(\"thesis_dataset1: done\")\n",
    "thesis_dataset2 = create_custom_dataset_with_sequences(nusc,'CAM_FRONT_RIGHT','RADAR_FRONT_RIGHT')\n",
    "print(\"thesis_dataset2: done\")\n",
    "thesis_dataset3 = create_custom_dataset_with_sequences(nusc,'CAM_FRONT_LEFT','RADAR_FRONT_LEFT')\n",
    "print(\"thesis_dataset3: done\")\n",
    "thesis_dataset4 = create_custom_dataset_with_sequences(nusc,'CAM_BACK_RIGHT','RADAR_BACK_RIGHT')\n",
    "print(\"thesis_dataset4: done\")\n",
    "thesis_dataset5 = create_custom_dataset_with_sequences(nusc,'CAM_BACK_LEFT','RADAR_BACK_LEFT')\n",
    "print(\"thesis_dataset5: done\")\n",
    "thesis_dataset6 = create_custom_dataset_with_sequences(nusc,'CAM_BACK','RADAR_BACK_RIGHT')\n",
    "print(\"thesis_dataset6: done\")\n",
    "thesis_dataset7 = create_custom_dataset_with_sequences(nusc,'CAM_BACK','RADAR_BACK_LEFT')\n",
    "print(\"thesis_dataset7: done\")\n",
    "thesis_dataset8 = create_custom_dataset_with_sequences(nusc,'CAM_BACK_LEFT','RADAR_FRONT_LEFT')\n",
    "print(\"thesis_dataset8: done\")\n",
    "thesis_dataset = thesis_dataset1 + thesis_dataset2 + thesis_dataset3 + thesis_dataset4 + thesis_dataset5 + thesis_dataset6 + thesis_dataset7 + thesis_dataset8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7d3f2b01-6322-4a70-8e39-319a8075c053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "912479"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(thesis_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "070af59d-1441-404e-94ce-89b2f862b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "DSsetn1 = thesis_dataset[:10000]\n",
    "#print(DSsetn1[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1009defe-5fbe-4053-9eee-fcc2849d4fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMAGE_SIZE = (224, 224)\n",
    "# Directory to save processed images\n",
    "image_dir = 'images'\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(image_path, save_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, IMAGE_SIZE)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    save_path_jpg = save_path if save_path.endswith('.jpg') else save_path + '.jpg'\n",
    "    cv2.imwrite(save_path_jpg, image)\n",
    "    return save_path_jpg\n",
    "\n",
    "# Function to transform bounding boxes\n",
    "def transform_bounding_boxes(bb_size, img_shape):\n",
    "    x1, y1, x2, y2 = bb_size\n",
    "    img_height, img_width = img_shape\n",
    "    width_ratio = img_width / 1600\n",
    "    height_ratio = img_height / 900\n",
    "    new_x1 = x1 * width_ratio\n",
    "    new_y1 = y1 * height_ratio\n",
    "    new_x2 = x2 * width_ratio\n",
    "    new_y2 = y2 * height_ratio\n",
    "    new_bbox_width = new_x2 - new_x1\n",
    "    new_bbox_height = new_y2 - new_y1\n",
    "    new_x_center = new_x1 + new_bbox_width / 2\n",
    "    new_y_center = new_y1 + new_bbox_height / 2\n",
    "    norm_x_center = new_x_center / img_width\n",
    "    norm_y_center = new_y_center / img_height\n",
    "    norm_width = new_bbox_width / img_width\n",
    "    norm_height = new_bbox_height / img_height\n",
    "    return [norm_x_center, norm_y_center, norm_width, norm_height]\n",
    "\n",
    "# Function to load and process dataset\n",
    "def load_dataset(dataset):\n",
    "    processed_datas = []\n",
    "\n",
    "    for data_point in tqdm(dataset):\n",
    "        processed_data = {'image_path': [],'bboxes': [],'dist_m': [],'speed_kmph': [],'classes': []}\n",
    "        image_path = data_point['image_data']\n",
    "        annotations = data_point['annotations']\n",
    "\n",
    "        save_path = os.path.join(image_dir, f\"processed_{os.path.basename(image_path)}\")\n",
    "        # Preprocess image\n",
    "        processed_image_path = preprocess_image(image_path, save_path)\n",
    "        #processed_image_path = '/kaggle/working/' + processed_image_path\n",
    "        processed_data['image_path'].append(processed_image_path)\n",
    "        \n",
    "        img_height, img_width = IMAGE_SIZE\n",
    "        for annotation in annotations:\n",
    "            bb_size = annotation['bb_size']\n",
    "            category_name = annotation['category_name']\n",
    "            distance = annotation['distance(m)']\n",
    "            speed = annotation['speed(km/hr)']\n",
    "            \n",
    "            # Transform bounding box\n",
    "            bb_transformed = transform_bounding_boxes(bb_size, (img_height, img_width))\n",
    "            \n",
    "            # One-hot encode class labels\n",
    "            one_hot_vector = np.zeros(8)\n",
    "            if category_name in category_map:\n",
    "                one_hot_vector[category_map[category_name]] = 1\n",
    "\n",
    "            processed_data['bboxes'].append(bb_transformed)\n",
    "            processed_data['dist_m'].append(distance)\n",
    "            processed_data['speed_kmph'].append(speed)\n",
    "            processed_data['classes'].append(one_hot_vector.tolist())\n",
    "\n",
    "        processed_datas.append(processed_data)\n",
    "\n",
    "    # Save the processed dataset to a JSON file\n",
    "    with open('processed_dataset.json', 'w') as json_file:\n",
    "        json.dump(processed_datas, json_file)\n",
    "    \n",
    "    return processed_datas\n",
    "\n",
    "# Example of category to index mapping (this needs to be created based on your dataset)\n",
    "category_map = {\n",
    "    'truck': 0,\n",
    "    'pedestrian': 1,\n",
    "    'bus': 2,\n",
    "    'car': 3,\n",
    "    'barrier': 4,\n",
    "    'trailer': 5,\n",
    "    'motorcycle': 6,\n",
    "    'bicycle': 7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fdf066-de39-43ed-869b-ee33c6d2d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "processed_dataset = load_dataset(DSsetn1)\n",
    "#processed_dataset[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1aeeae3-2327-4448-9120-4f825df9ecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install keras-tuner -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecde9043-e62c-447e-b39e-b2d4f7f9adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file\n",
    "with open('processed_dataset.json', 'r') as json_file:\n",
    "    processed_dataset = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "58b02b3a-0c4e-4bde-80ae-cb110182821c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(processed_dataset))\n",
    "\n",
    "max_bboxes_per_image = max(len(bb) for frame_data in processed_dataset for bb in frame_data['bboxes'])\n",
    "\n",
    "print(max_bboxes_per_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3628fece-b83b-4040-8bfc-eb9312bc672b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset element shapes: ((TensorSpec(shape=(None, 20, 4), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32, name=None)), (TensorSpec(shape=(None, 20), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20), dtype=tf.float32, name=None)))\n",
      "Validation dataset element shapes: ((TensorSpec(shape=(None, 20, 4), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32, name=None)), (TensorSpec(shape=(None, 20), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20), dtype=tf.float32, name=None)))\n",
      "Test dataset element shapes: ((TensorSpec(shape=(None, 20, 4), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32, name=None)), (TensorSpec(shape=(None, 20), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20), dtype=tf.float32, name=None)))\n"
     ]
    }
   ],
   "source": [
    "# Helper function to load and preprocess images\n",
    "def load_image(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = np.expand_dims(img, axis=-1)  # Add channel dimension\n",
    "    img = img.astype('float32') / 255.0  # Normalize to [0, 1]\n",
    "    return img\n",
    "\n",
    "# Prepare the data separately\n",
    "def prepare_data(data):\n",
    "    images = []\n",
    "    bboxes = []\n",
    "    classes = []\n",
    "    distances = []\n",
    "    speeds = []\n",
    "\n",
    "    for item in data:\n",
    "        image_path = item['image_path'][0]\n",
    "        image = load_image(image_path)\n",
    "        images.append(image)\n",
    "        \n",
    "        bboxes.append(np.array(item['bboxes'], dtype='float32'))\n",
    "        distances.append(np.array(item['dist_m'], dtype='float32'))\n",
    "        speeds.append(np.array(item['speed_kmph'], dtype='float32'))\n",
    "        classes.append(np.array(item['classes'], dtype='float32'))\n",
    "\n",
    "    # Convert lists to arrays\n",
    "    images = np.array(images)\n",
    "\n",
    "    # Bounding boxes, distances, speeds, and classes might have different lengths, so we need to pad them\n",
    "    max_len = 20  # Maximum number of bounding boxes (and related arrays)\n",
    "    \n",
    "    bboxes_padded = tf.keras.preprocessing.sequence.pad_sequences(bboxes, maxlen=max_len, padding='post', dtype='float32', value=-1)\n",
    "    classes_padded = tf.keras.preprocessing.sequence.pad_sequences(classes, maxlen=max_len, padding='post', dtype='float32', value=-1)\n",
    "    distances_padded = tf.keras.preprocessing.sequence.pad_sequences(distances, maxlen=max_len, padding='post', dtype='float32', value=-1)\n",
    "    speeds_padded = tf.keras.preprocessing.sequence.pad_sequences(speeds, maxlen=max_len, padding='post', dtype='float32', value=-1)\n",
    "\n",
    "    return images, bboxes_padded, classes_padded, distances_padded, speeds_padded\n",
    "\n",
    "# Prepare the data\n",
    "images, bboxes, classes, distances, speeds = prepare_data(processed_dataset)\n",
    "\n",
    "batch_size = 7\n",
    "\n",
    "# Create TensorFlow Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((( bboxes, classes, images), (distances, speeds)))\n",
    "dataset = dataset.shuffle(buffer_size=len(images))\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_size = int(0.7 * len(images))\n",
    "val_size = int(0.2 * len(images))\n",
    "test_size = len(images) - train_size - val_size\n",
    "\n",
    "train_dataset = dataset.take(train_size).batch(batch_size)\n",
    "val_dataset = dataset.skip(train_size).take(val_size).batch(batch_size)\n",
    "test_dataset = dataset.skip(train_size + val_size).batch(batch_size)\n",
    "\n",
    "\n",
    "# Check shapes\n",
    "print(\"Train dataset element shapes:\", train_dataset.element_spec)\n",
    "print(\"Validation dataset element shapes:\", val_dataset.element_spec)\n",
    "print(\"Test dataset element shapes:\", test_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da14f1ea-7584-4f76-bcef-ac40b4e6328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import VGG16\n",
    "import keras_tuner as kt\n",
    "#from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Enable mixed precision\n",
    "#mixed_precision.set_global_policy('mixed_float16')\n",
    "def masked_mae(y_true, y_pred):\n",
    "    mask = tf.not_equal(y_true, -1)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mae = tf.abs(y_true - y_pred)\n",
    "    masked_mae = tf.multiply(mae, mask)\n",
    "    return tf.reduce_sum(masked_mae) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    inputs = {\n",
    "        'image_path': layers.Input(shape=(224, 224, 1), name='image_path'),\n",
    "        'bboxes': layers.Input(shape=(20, 4), name='bboxes'),\n",
    "        'classes': layers.Input(shape=(20, 8), name='classes')\n",
    "    }\n",
    "\n",
    "    # Convert grayscale images to 3 channels to match VGG16 input requirements\n",
    "    images_3ch = layers.Concatenate()([inputs['image_path']] * 3)\n",
    "\n",
    "    # Load VGG16 model without the top layers and with pretrained weights\n",
    "    vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=images_3ch)\n",
    "    \n",
    "    # Freeze VGG16 layers\n",
    "    for layer in vgg16.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "     # Add additional convolutional layers\n",
    "    x = vgg16.output\n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    # Define a mask for the input sequences (bboxes and classes)\n",
    "    bbox_mask = tf.keras.layers.Masking(mask_value=-1)(inputs['bboxes'])\n",
    "    class_mask = tf.keras.layers.Masking(mask_value=-1)(inputs['classes'])\n",
    "\n",
    "        # Flatten the masked sequences\n",
    "    flat_bboxes = layers.Flatten()(bbox_mask)\n",
    "    flat_classes = layers.Flatten()(class_mask)\n",
    "\n",
    "    # Concatenate all inputs\n",
    "    concatenated = layers.Concatenate()([x, flat_bboxes, flat_classes])\n",
    "    # Tune the number of dense layers\n",
    "    num_dense_layers = hp.Int('num_dense_layers', min_value=8, max_value=11, step=1)\n",
    "    for i in range(num_dense_layers):\n",
    "        units = hp.Int(f'dense_units_{i}', min_value=256, max_value=1762, step=64)\n",
    "        y = layers.Dense(units, activation='relu')(concatenated)\n",
    "        dropout_rate = hp.Float(f'dropout_rate_{i}', min_value=0.1, max_value=0.5, step=0.1)\n",
    "        y = layers.Dropout(dropout_rate)(y)\n",
    "\n",
    "    # Output layers\n",
    "    output_distances = layers.Dense(20, name='dist_m')(y)\n",
    "    output_speeds = layers.Dense(20, name='speed_kmph')(y)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=[output_distances, output_speeds])\n",
    "\n",
    "    # Tune the learning rate\n",
    "    #learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss={\n",
    "            'dist_m': masked_mae,\n",
    "            'speed_kmph': masked_mae\n",
    "        },\n",
    "        metrics={\n",
    "            'dist_m': masked_mae,\n",
    "            'speed_kmph': masked_mae\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "#Instantiate the tuner with early stopping and parallel trails\n",
    "\n",
    "# Instantiate the tuner with Hyperband\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=150,\n",
    "    factor=3,\n",
    "    directory='DSpredict2',\n",
    "    project_name='DSM_tuning',\n",
    "    overwrite=False\n",
    "   # distribution_strategy=tf.distribute.MirroredStrategy()\n",
    ")\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "#tuner.search_space_summary()#Search for the best hyperparameters\n",
    "\n",
    "tuner.search(train_dataset, epochs=50, validation_data=val_dataset, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a49c58e-bc54-4b75-a251-f9b747b2f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import VGG16\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Enable mixed precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "def build_model(hp):\n",
    "    inputs = {\n",
    "        'image_path': layers.Input(shape=(224, 224, 1), name='image_path'),\n",
    "        'bboxes': layers.Input(shape=(20, 4), name='bboxes'),\n",
    "        'classes': layers.Input(shape=(20, 8), name='classes')\n",
    "    }\n",
    "\n",
    "    # Convert grayscale images to 3 channels to match VGG16 input requirements\n",
    "    images_3ch = layers.Concatenate()([inputs['image_path']] * 3)\n",
    "    # Load VGG16 model without the top layers and with pretrained weights\n",
    "    vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=images_3ch)\n",
    "    \n",
    "    # Freeze VGG16 layers\n",
    "    for layer in vgg16.layers:\n",
    "        layer.trainable = False\n",
    "    # Add additional convolutional layers\n",
    "    x = vgg16.output\n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    # Define a mask for the input sequences (bboxes and classes)\n",
    "    bbox_mask = layers.Masking(mask_value=-1)(inputs['bboxes'])\n",
    "    class_mask = layers.Masking(mask_value=-1)(inputs['classes'])\n",
    "\n",
    "    # Concatenate all inputs\n",
    "    concatenated = layers.Concatenate()([x, layers.Flatten()(inputs['bboxes']), layers.Flatten()(inputs['classes'])])\n",
    "\n",
    "    # Tune the number of dense layers\n",
    "    num_dense_layers = hp.Int('num_dense_layers', min_value=9, max_value=15, step=1)\n",
    "    for i in range(num_dense_layers):\n",
    "        units = hp.Int(f'dense_units_{i}', min_value=256, max_value=2048, step=64)\n",
    "        y = layers.Dense(units, activation='relu', kernel_regularizer=l2(0.001))(concatenated)\n",
    "        y = BatchNormalization()(y)\n",
    "        dropout_rate = hp.Float(f'dropout_rate_{i}', min_value=0.1, max_value=0.5, step=0.1)\n",
    "        y = layers.Dropout(dropout_rate)(y)\n",
    "\n",
    "    # Output layers\n",
    "    output_distances = layers.Dense(20, name='dist_m')(y)\n",
    "    output_speeds = layers.Dense(20, name='speed_kmph')(y)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=[output_distances, output_speeds])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss={'dist_m': masked_mae,\n",
    "            'speed_kmph': masked_mae},\n",
    "        metrics={'dist_m': masked_mae,\n",
    "            'speed_kmph': masked_mae}\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Instantiate the tuner with Hyperband\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=50,\n",
    "    factor=3,\n",
    "    directory='DSpredict2',\n",
    "    project_name='DSM_tuning',\n",
    "    overwrite=False\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "tuner.search(train_dataset, epochs=50, validation_data=val_dataset, callbacks=[early_stopping, lr_schedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "49eb75a5-a4e0-4e28-b381-b2b8a3d694e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "95bb4fd0-1d36-4f5f-8866-3e710180ff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "trial_id = best_trial.trial_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb8953-f60e-44a7-934c-bb5d89d6da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trial_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dc1c75-8246-488e-aab3-ac37826b5d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "best_model.fit(train_dataset, epochs=150, validation_data=val_dataset, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "12b20d1e-bb3c-4bc1-8c93-27120cf8ff38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "best_model.save('dsp4.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e4bb9949-42c1-4a34-9e55-4df3945021b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load the best model with custom_objects specified\n",
    "custom_objects = {\n",
    "    'mae': tf.keras.metrics.MeanAbsoluteError\n",
    "}\n",
    "\n",
    "best_model = tf.keras.models.load_model('trained_dsp1_same.h5', custom_objects=custom_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9404111e-b0fa-487e-94e8-ada9f894ef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss={\n",
    "            'dist_m': masked_mae,\n",
    "            'speed_kmph': masked_mae\n",
    "        },\n",
    "        metrics={\n",
    "            'dist_m': masked_mae,\n",
    "            'speed_kmph': masked_mae\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b9a14e-af87-4ff5-acc7-de873740c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "best_model.fit(train_dataset, epochs=150, validation_data=val_dataset, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "13f4d340-a9f9-4033-9640-872454357deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 161ms/step - dist_m_masked_mae: 2.6083 - loss: 3.7954 - speed_kmph_masked_mae: 1.1870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.847412109375, 2.610424041748047, 1.236154317855835]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "51c92045-77c6-44ae-b03f-db01022b4eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 159ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[23.293066  ,  8.259662  , 12.715488  , ..., -5.1908264 ,\n",
       "         -1.3025492 ,  3.2110267 ],\n",
       "        [14.450374  , 15.031685  , 11.613392  , ..., -7.668827  ,\n",
       "          0.59965074,  1.321842  ],\n",
       "        [12.738454  , 13.128754  , 21.393763  , ..., -6.454669  ,\n",
       "          2.3647768 ,  2.756866  ],\n",
       "        ...,\n",
       "        [15.937577  , 24.212788  , 28.075754  , ..., -4.3610315 ,\n",
       "          0.35079134, -2.2857049 ],\n",
       "        [43.646004  , 16.425417  , 19.392088  , ..., -8.235331  ,\n",
       "         -5.0320673 ,  4.7646713 ],\n",
       "        [12.005981  , 15.925512  , 15.69592   , ..., -1.522191  ,\n",
       "          7.182911  , -5.1583357 ]], dtype=float32),\n",
       " array([[-0.24478158,  0.9858184 ,  4.6937623 , ...,  2.0507824 ,\n",
       "         -1.5551109 , -0.55669093],\n",
       "        [ 1.0076264 ,  2.4399936 ,  0.32026055, ..., -1.7345837 ,\n",
       "         -0.46959662, -0.9613494 ],\n",
       "        [ 0.10431685,  1.715667  ,  0.56577593, ...,  0.7687601 ,\n",
       "          1.7919552 ,  2.4159484 ],\n",
       "        ...,\n",
       "        [ 2.2276416 ,  4.1495547 ,  1.9363706 , ...,  2.3787184 ,\n",
       "         -1.7369936 ,  5.7514434 ],\n",
       "        [-0.22403288,  0.9417563 ,  0.84243906, ..., -0.3030653 ,\n",
       "         -0.72447443,  3.7911432 ],\n",
       "        [12.940746  ,  2.7816253 ,  1.3354456 , ...,  0.28301892,\n",
       "         -4.0455494 , -1.7030673 ]], dtype=float32)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d5965e46-2f83-4c7d-936f-2eb3c96f042e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "best_model.save('dsp3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04aba4-9fbf-4269-a8a9-a2797680f3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0b95049-74c6-42e6-8500-497441cde7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the best model with custom_objects specified\n",
    "custom_objects = { 'mae': tf.keras.metrics.MeanAbsoluteError}\n",
    "\n",
    "best_model = tf.keras.models.load_model('trained_dsp1hp.h5', custom_objects=custom_objects)\n",
    "\n",
    "# Save the model as a TensorFlow SavedModel\n",
    "#tf.saved_model.save(best_model, 'saved_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14899f9-d960-4075-9229-ed5fbd9a39ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to TFLite with post-training dynamic range quantization\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('saved_model')\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the quantized model to a file\n",
    "with open('dsp_model_quantized.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a11bee1f-f25a-4411-b53a-6aa4a185d216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ image_path          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ image_path[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │ image_path[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ image_path[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block1_conv1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block1_conv2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ block1_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block1_pool         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block1_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block2_conv1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ block1_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block2_conv2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>,  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ block2_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block2_pool         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block2_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block3_conv1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │ block2_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block3_conv2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │ block3_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block3_conv3        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │ block3_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block3_pool         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block3_conv3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block4_conv1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │ block3_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block4_conv2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │ block4_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block4_conv3        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │ block4_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block4_pool         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block4_conv3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block5_conv1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │ block4_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block5_conv2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │ block5_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block5_conv3        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │ block5_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block5_pool         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block5_conv3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bboxes (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ classes             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ block5_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bboxes[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ classes[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25328</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ flatten_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ flatten_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">38,905,344</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dist_m (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">30,740</span> │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ speed_kmph (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">30,740</span> │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ image_path          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m1\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ image_path[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m3\u001b[0m)                │            │ image_path[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ image_path[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block1_conv1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │      \u001b[38;5;34m1,792\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block1_conv2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │     \u001b[38;5;34m36,928\u001b[0m │ block1_conv1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block1_pool         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ block1_conv2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block2_conv1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m,  │     \u001b[38;5;34m73,856\u001b[0m │ block1_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block2_conv2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m,  │    \u001b[38;5;34m147,584\u001b[0m │ block2_conv1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block2_pool         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ block2_conv2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block3_conv1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    │    \u001b[38;5;34m295,168\u001b[0m │ block2_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block3_conv2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    │    \u001b[38;5;34m590,080\u001b[0m │ block3_conv1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block3_conv3        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    │    \u001b[38;5;34m590,080\u001b[0m │ block3_conv2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block3_pool         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ block3_conv3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block4_conv1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    │  \u001b[38;5;34m1,180,160\u001b[0m │ block3_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block4_conv2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    │  \u001b[38;5;34m2,359,808\u001b[0m │ block4_conv1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block4_conv3        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    │  \u001b[38;5;34m2,359,808\u001b[0m │ block4_conv2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block4_pool         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ block4_conv3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block5_conv1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │  \u001b[38;5;34m2,359,808\u001b[0m │ block4_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block5_conv2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │  \u001b[38;5;34m2,359,808\u001b[0m │ block5_conv1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block5_conv3        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │  \u001b[38;5;34m2,359,808\u001b[0m │ block5_conv2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2D\u001b[0m)            │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ block5_pool         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ block5_conv3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bboxes (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ classes             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m8\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ block5_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ bboxes[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ classes[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25328\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ flatten_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ flatten_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)      │ \u001b[38;5;34m38,905,344\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dist_m (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │     \u001b[38;5;34m30,740\u001b[0m │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ speed_kmph (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │     \u001b[38;5;34m30,740\u001b[0m │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,681,512</span> (204.78 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m53,681,512\u001b[0m (204.78 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">38,966,824</span> (148.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m38,966,824\u001b[0m (148.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da1f54e4-fba8-4386-86ce-f4c816788d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in DSpredict2/DSM_tuning\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 0136 summary\n",
      "Hyperparameters:\n",
      "num_dense_layers: 9\n",
      "dense_units_0: 256\n",
      "dropout_rate_0: 0.1\n",
      "dense_units_1: 256\n",
      "dropout_rate_1: 0.4\n",
      "dense_units_2: 640\n",
      "dropout_rate_2: 0.2\n",
      "dense_units_3: 1408\n",
      "dropout_rate_3: 0.2\n",
      "dense_units_4: 640\n",
      "dropout_rate_4: 0.5\n",
      "dense_units_5: 1536\n",
      "dropout_rate_5: 0.4\n",
      "dense_units_6: 960\n",
      "dropout_rate_6: 0.1\n",
      "dense_units_7: 384\n",
      "dropout_rate_7: 0.4\n",
      "dense_units_8: 1536\n",
      "dropout_rate_8: 0.1\n",
      "tuner/epochs: 17\n",
      "tuner/initial_epoch: 6\n",
      "tuner/bracket: 4\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0123\n",
      "Score: 0.5195848345756531\n",
      "\n",
      "Trial 0133 summary\n",
      "Hyperparameters:\n",
      "num_dense_layers: 7\n",
      "dense_units_0: 1216\n",
      "dropout_rate_0: 0.1\n",
      "dense_units_1: 768\n",
      "dropout_rate_1: 0.1\n",
      "dense_units_2: 384\n",
      "dropout_rate_2: 0.2\n",
      "dense_units_3: 1664\n",
      "dropout_rate_3: 0.4\n",
      "dense_units_4: 256\n",
      "dropout_rate_4: 0.1\n",
      "dense_units_5: 1216\n",
      "dropout_rate_5: 0.5\n",
      "dense_units_6: 1152\n",
      "dropout_rate_6: 0.1\n",
      "dense_units_7: 896\n",
      "dropout_rate_7: 0.1\n",
      "dense_units_8: 512\n",
      "dropout_rate_8: 0.5\n",
      "tuner/epochs: 17\n",
      "tuner/initial_epoch: 6\n",
      "tuner/bracket: 4\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0104\n",
      "Score: 0.5518057346343994\n",
      "\n",
      "Trial 0138 summary\n",
      "Hyperparameters:\n",
      "num_dense_layers: 7\n",
      "dense_units_0: 576\n",
      "dropout_rate_0: 0.30000000000000004\n",
      "dense_units_1: 384\n",
      "dropout_rate_1: 0.2\n",
      "dense_units_2: 320\n",
      "dropout_rate_2: 0.2\n",
      "dense_units_3: 384\n",
      "dropout_rate_3: 0.5\n",
      "dense_units_4: 1472\n",
      "dropout_rate_4: 0.5\n",
      "dense_units_5: 960\n",
      "dropout_rate_5: 0.2\n",
      "dense_units_6: 1664\n",
      "dropout_rate_6: 0.1\n",
      "dense_units_7: 1088\n",
      "dropout_rate_7: 0.2\n",
      "dense_units_8: 896\n",
      "dropout_rate_8: 0.30000000000000004\n",
      "tuner/epochs: 17\n",
      "tuner/initial_epoch: 6\n",
      "tuner/bracket: 4\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0122\n",
      "Score: 0.5615137815475464\n",
      "\n",
      "Trial 0135 summary\n",
      "Hyperparameters:\n",
      "num_dense_layers: 5\n",
      "dense_units_0: 896\n",
      "dropout_rate_0: 0.1\n",
      "dense_units_1: 512\n",
      "dropout_rate_1: 0.1\n",
      "dense_units_2: 1216\n",
      "dropout_rate_2: 0.1\n",
      "dense_units_3: 640\n",
      "dropout_rate_3: 0.2\n",
      "dense_units_4: 1344\n",
      "dropout_rate_4: 0.1\n",
      "dense_units_5: 1536\n",
      "dropout_rate_5: 0.30000000000000004\n",
      "dense_units_6: 576\n",
      "dropout_rate_6: 0.2\n",
      "dense_units_7: 768\n",
      "dropout_rate_7: 0.5\n",
      "dense_units_8: 1536\n",
      "dropout_rate_8: 0.1\n",
      "tuner/epochs: 17\n",
      "tuner/initial_epoch: 6\n",
      "tuner/bracket: 4\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0110\n",
      "Score: 0.572218656539917\n",
      "\n",
      "Trial 0140 summary\n",
      "Hyperparameters:\n",
      "num_dense_layers: 7\n",
      "dense_units_0: 1280\n",
      "dropout_rate_0: 0.2\n",
      "dense_units_1: 832\n",
      "dropout_rate_1: 0.1\n",
      "dense_units_2: 576\n",
      "dropout_rate_2: 0.2\n",
      "dense_units_3: 1728\n",
      "dropout_rate_3: 0.5\n",
      "dense_units_4: 768\n",
      "dropout_rate_4: 0.1\n",
      "dense_units_5: 1280\n",
      "dropout_rate_5: 0.1\n",
      "dense_units_6: 1536\n",
      "dropout_rate_6: 0.1\n",
      "dense_units_7: 768\n",
      "dropout_rate_7: 0.30000000000000004\n",
      "dense_units_8: 1664\n",
      "dropout_rate_8: 0.1\n",
      "tuner/epochs: 17\n",
      "tuner/initial_epoch: 6\n",
      "tuner/bracket: 4\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0098\n",
      "Score: 0.5759005546569824\n",
      "\n",
      "Trial 0132 summary\n",
      "Hyperparameters:\n",
      "num_dense_layers: 5\n",
      "dense_units_0: 1728\n",
      "dropout_rate_0: 0.4\n",
      "dense_units_1: 832\n",
      "dropout_rate_1: 0.30000000000000004\n",
      "dense_units_2: 384\n",
      "dropout_rate_2: 0.2\n",
      "dense_units_3: 1536\n",
      "dropout_rate_3: 0.2\n",
      "dense_units_4: 832\n",
      "dropout_rate_4: 0.1\n",
      "dense_units_5: 1344\n",
      "dropout_rate_5: 0.2\n",
      "dense_units_6: 448\n",
      "dropout_rate_6: 0.30000000000000004\n",
      "dense_units_7: 448\n",
      "dropout_rate_7: 0.5\n",
      "dense_units_8: 1600\n",
      "dropout_rate_8: 0.5\n",
      "tuner/epochs: 17\n",
      "tuner/initial_epoch: 6\n",
      "tuner/bracket: 4\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0105\n",
      "Score: 0.5839475989341736\n",
      "\n",
      "Trial 0137 summary\n",
      "Hyperparameters:\n",
      "num_dense_layers: 5\n",
      "dense_units_0: 576\n",
      "dropout_rate_0: 0.1\n",
      "dense_units_1: 704\n",
      "dropout_rate_1: 0.4\n",
      "dense_units_2: 1472\n",
      "dropout_rate_2: 0.4\n",
      "dense_units_3: 768\n",
      "dropout_rate_3: 0.30000000000000004\n",
      "dense_units_4: 832\n",
      "dropout_rate_4: 0.1\n",
      "dense_units_5: 640\n",
      "dropout_rate_5: 0.2\n",
      "dense_units_6: 1472\n",
      "dropout_rate_6: 0.1\n",
      "dense_units_7: 448\n",
      "dropout_rate_7: 0.30000000000000004\n",
      "dense_units_8: 1024\n",
      "dropout_rate_8: 0.4\n",
      "tuner/epochs: 17\n",
      "tuner/initial_epoch: 6\n",
      "tuner/bracket: 4\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0115\n",
      "Score: 0.5881933569908142\n",
      "\n",
      "Trial 0131 summary\n",
      "Hyperparameters:\n",
      "num_dense_layers: 9\n",
      "dense_units_0: 1088\n",
      "dropout_rate_0: 0.30000000000000004\n",
      "dense_units_1: 256\n",
      "dropout_rate_1: 0.1\n",
      "dense_units_2: 448\n",
      "dropout_rate_2: 0.30000000000000004\n",
      "dense_units_3: 448\n",
      "dropout_rate_3: 0.4\n",
      "dense_units_4: 384\n",
      "dropout_rate_4: 0.2\n",
      "dense_units_5: 960\n",
      "dropout_rate_5: 0.1\n",
      "dense_units_6: 1472\n",
      "dropout_rate_6: 0.5\n",
      "dense_units_7: 768\n",
      "dropout_rate_7: 0.1\n",
      "dense_units_8: 1024\n",
      "dropout_rate_8: 0.1\n",
      "tuner/epochs: 17\n",
      "tuner/initial_epoch: 6\n",
      "tuner/bracket: 4\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0106\n",
      "Score: 0.5972306132316589\n",
      "\n",
      "Trial 0139 summary\n",
      "Hyperparameters:\n",
      "num_dense_layers: 7\n",
      "dense_units_0: 1536\n",
      "dropout_rate_0: 0.30000000000000004\n",
      "dense_units_1: 320\n",
      "dropout_rate_1: 0.1\n",
      "dense_units_2: 448\n",
      "dropout_rate_2: 0.1\n",
      "dense_units_3: 1024\n",
      "dropout_rate_3: 0.5\n",
      "dense_units_4: 1728\n",
      "dropout_rate_4: 0.4\n",
      "dense_units_5: 1728\n",
      "dropout_rate_5: 0.2\n",
      "dense_units_6: 640\n",
      "dropout_rate_6: 0.1\n",
      "dense_units_7: 1280\n",
      "dropout_rate_7: 0.5\n",
      "dense_units_8: 896\n",
      "dropout_rate_8: 0.1\n",
      "tuner/epochs: 17\n",
      "tuner/initial_epoch: 6\n",
      "tuner/bracket: 4\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0111\n",
      "Score: 0.6390660405158997\n",
      "\n",
      "Trial 0134 summary\n",
      "Hyperparameters:\n",
      "num_dense_layers: 9\n",
      "dense_units_0: 640\n",
      "dropout_rate_0: 0.2\n",
      "dense_units_1: 1536\n",
      "dropout_rate_1: 0.2\n",
      "dense_units_2: 832\n",
      "dropout_rate_2: 0.1\n",
      "dense_units_3: 960\n",
      "dropout_rate_3: 0.5\n",
      "dense_units_4: 256\n",
      "dropout_rate_4: 0.4\n",
      "dense_units_5: 1472\n",
      "dropout_rate_5: 0.4\n",
      "dense_units_6: 1024\n",
      "dropout_rate_6: 0.4\n",
      "dense_units_7: 1664\n",
      "dropout_rate_7: 0.2\n",
      "dense_units_8: 1344\n",
      "dropout_rate_8: 0.1\n",
      "tuner/epochs: 17\n",
      "tuner/initial_epoch: 6\n",
      "tuner/bracket: 4\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0101\n",
      "Score: 0.6492130160331726\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f9441-5957-4867-80a5-12a7ad6fc02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the processed dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_processed_data(processed_data):\n",
    "    for data_point in processed_data:\n",
    "        image_path = data_point['image_path']\n",
    "        bbox = data_point['bbox']\n",
    "        distance = data_point['distance']\n",
    "        speed = data_point['speed']\n",
    "        class_label = np.argmax(data_point['class'])\n",
    "        \n",
    "        # Load the preprocessed image\n",
    "        image = np.load(image_path)\n",
    "        \n",
    "        # Display image and bounding box\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.scatter(bbox[0] * IMAGE_SIZE[1], bbox[1] * IMAGE_SIZE[0], c='red')  # center point\n",
    "        plt.gca().add_patch(plt.Rectangle((bbox[0] * IMAGE_SIZE[1] - bbox[2] * IMAGE_SIZE[1] / 2,\n",
    "                                           bbox[1] * IMAGE_SIZE[0] - bbox[3] * IMAGE_SIZE[0] / 2),\n",
    "                                           bbox[2] * IMAGE_SIZE[1],\n",
    "                                           bbox[3] * IMAGE_SIZE[0],\n",
    "                                           edgecolor='blue', facecolor='none'))\n",
    "        plt.title(f'Class: {class_label}, Distance: {distance:.2f}m, Speed: {speed:.2f}km/h')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize the first data point in the processed dataset\n",
    "visualize_processed_data(processed_dataset[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7bdc43ff-b57a-42e5-87ec-dc264e290f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file\n",
    "with open('processed_dataset.json', 'r') as json_file:\n",
    "    processed1 = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "45533c56-ce0a-4e7d-ad8e-3bc6659e8172",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[178], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mprocessed1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "print(processed1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "84eb6b9e-5d89-49af-a898-2f54d3e536b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save processed images\n",
    "processed_image_dir = 'images'\n",
    "os.makedirs(processed_image_dir, exist_ok=True)\n",
    "\n",
    "# Function to convert image to grayscale, resize, normalize, and save as .npy\n",
    "def load_and_normalize_image(image_path, save_path, new_width, new_height):\n",
    "    # Load the image using cv2\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Image not found at path: {image_path}\")\n",
    "    # Resize the image\n",
    "    img_resized = cv2.resize(img, (new_width, new_height))\n",
    "    # Convert to grayscale\n",
    "    gray_img = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
    "    # Save the processed image\n",
    "    np.save(save_path, gray_img)\n",
    "    return normalized_img\n",
    "\n",
    "# Function to convert bounding boxes to (xc, yc, w, h) format\n",
    "def convert_bboxes(bboxes, width_ratio, height_ratio, new_width, new_height):\n",
    "    adjusted_bboxes = []\n",
    "    for bbox in bboxes:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        new_x1 = x1 * width_ratio\n",
    "        new_y1 = y1 * height_ratio\n",
    "        new_x2 = x2 * width_ratio\n",
    "        new_y2 = y2 * height_ratio\n",
    "        new_bbox_width = new_x2 - new_x1\n",
    "        new_bbox_height = new_y2 - new_y1\n",
    "        new_x_center = new_x1 + new_bbox_width / 2\n",
    "        new_y_center = new_y1 + new_bbox_height / 2\n",
    "        norm_x_center = new_x_center / new_width\n",
    "        norm_y_center = new_y_center / new_height\n",
    "        norm_width = new_bbox_width / new_width\n",
    "        norm_height = new_bbox_height / new_height\n",
    "        adjusted_bboxes.append((norm_x_center, norm_y_center, norm_width, norm_height))\n",
    "    return adjusted_bboxes\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess_dataset(dataset, new_width, new_height, num_classes):\n",
    "    processed_sequences = []\n",
    "\n",
    "    for sequence_data in dataset:\n",
    "        processed_sequence = {'image_paths': [], 'bboxes': [], 'classes': [], 'distances': [], 'speeds': []}\n",
    "        for frame_data in sequence_data['sequence']:\n",
    "            image_path = frame_data['image_data']\n",
    "            annotations = frame_data['annotations']\n",
    "\n",
    "            bboxes = [ann['bb_size'] for ann in annotations]\n",
    "            categories = [ann['category_name'] for ann in annotations]\n",
    "            distances = [ann['distance(m)'] for ann in annotations]\n",
    "            speeds = [ann['speed(km/hr)'] for ann in annotations]\n",
    "\n",
    "            # Define path to save the processed image\n",
    "            save_path = os.path.join(processed_image_dir, f\"processed_{os.path.basename(image_path)}\")\n",
    "            \n",
    "            # Convert and normalize image\n",
    "            img_array = load_and_normalize_image(image_path, save_path, new_width, new_height)\n",
    "            new_height, new_width = img_array.shape[:2]\n",
    "\n",
    "            # Calculate new bounding box coordinates based on the resized image\n",
    "            width_ratio = new_width / 1600\n",
    "            height_ratio = new_height / 900\n",
    "  \n",
    "            adjusted_bboxes = convert_bboxes(bboxes, width_ratio, height_ratio, new_width, new_height)\n",
    "\n",
    "            save_path = '/kaggle/working/' + save_path\n",
    "\n",
    "            processed_sequence['image_paths'].append(save_path)\n",
    "\n",
    "            # One-hot encode the classes\n",
    "            class_labels = np.zeros((len(categories), num_classes))\n",
    "            for idx, category in enumerate(categories):\n",
    "                class_labels[idx][category_index_map[category]] = 1  # `category_index_map` maps category names to indices\n",
    "            processed_sequence['classes'].append(class_labels.tolist())  # Convert to list for JSON serialization\n",
    "            processed_sequence['bboxes'].append(adjusted_bboxes)\n",
    "            processed_sequence['distances'].append(distances)\n",
    "            processed_sequence['speeds'].append(speeds)\n",
    "\n",
    "        processed_sequences.append(processed_sequence)\n",
    "\n",
    "    # Save the processed sequences to a JSON file\n",
    "    with open('processed_dataset.json', 'w') as json_file:\n",
    "        json.dump(processed_sequences, json_file)\n",
    "\n",
    "    return processed_sequences\n",
    "\n",
    "# Example of category to index mapping (this needs to be created based on your dataset)\n",
    "category_index_map = {\n",
    "    'truck': 0,\n",
    "    'pedestrian': 1,\n",
    "    'bus': 2,\n",
    "    'car': 3,\n",
    "    'barrier': 4,\n",
    "    'trailer': 5,\n",
    "    'motorcycle': 6,\n",
    "    'bicycle': 7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4a5d1a53-a13b-4342-9c19-4111abff63d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process your dataset\n",
    "new_width, new_height = 224, 224\n",
    "num_classes = len(category_index_map)\n",
    "processed_DSsetn1 = preprocess_dataset(DSsetn1, new_width, new_height, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "201e1710-7e63-448f-8637-b84a9adf714d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_paths': ['/kaggle/working/images/processed_n015-2018-07-18-11-07-57+0800__CAM_FRONT__1531883530412470.jpg.npy',\n",
       "  '/kaggle/working/images/processed_n015-2018-07-18-11-07-57+0800__CAM_FRONT__1531883530912460.jpg.npy',\n",
       "  '/kaggle/working/images/processed_n015-2018-07-18-11-07-57+0800__CAM_FRONT__1531883531412477.jpg.npy'],\n",
       " 'bboxes': [[(0.10705136644342253,\n",
       "    0.53595261176308,\n",
       "    0.21410273288684506,\n",
       "    0.27694355615032207)],\n",
       "  [(0.20594846081371138,\n",
       "    0.5483424302949975,\n",
       "    0.25154022870696474,\n",
       "    0.2965443159572492),\n",
       "   (0.05507065320647937,\n",
       "    0.552458969106816,\n",
       "    0.11014130641295874,\n",
       "    0.16162240011243018)],\n",
       "  [(0.3566807446720645,\n",
       "    0.5468613398454547,\n",
       "    0.2499268071317405,\n",
       "    0.3247820843621831),\n",
       "   (0.19628845959942076,\n",
       "    0.5510165732607867,\n",
       "    0.13467437734882476,\n",
       "    0.16070878095022184)]],\n",
       " 'classes': [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]],\n",
       "  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "   [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]],\n",
       "  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "   [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]],\n",
       " 'distances': [[18.768916998713365],\n",
       "  [16.96150745446025, 27.995598918751572],\n",
       "  [14.438545649347727, 25.854386035199195]],\n",
       " 'speeds': [[0.5392347791087571],\n",
       "  [0.7520438424393308, 0.7209372320174793],\n",
       "  [0.46239326302349054, 1.1032632010257906]]}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_DSsetn1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc99ee1b-2942-4128-950e-7d7f2a8ced28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: processed_dataset.json (deflated 70%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r processed_dataset.json.zip processed_dataset.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d09904-9d4f-4f4c-90c5-0fd808461408",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r images.zip images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030298dd-85ce-40fc-943c-1cab3adae9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3ae5c24a-87ae-42fd-bfb3-f6ac7233546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update image paths\n",
    "def update_image_paths(dataset):\n",
    "    for data in dataset:\n",
    "        data['image_paths'] = [path.replace('/kaggle/working/', '') for path in data['image_paths']]\n",
    "    return dataset\n",
    "\n",
    "processed_DSsetn1 = update_image_paths(processed_DSsetn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c2213-84d4-426c-9a97-32328b163be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ndarray(obj):\n",
    "    if isinstance(obj, list):\n",
    "        try:\n",
    "            return np.array(obj)\n",
    "        except ValueError:\n",
    "            # If the conversion fails, it means the list is not directly convertible to a numpy array\n",
    "            return [convert_to_ndarray(element) for element in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_ndarray(value) for key, value in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def load_from_json(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "    return convert_to_ndarray(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d65a49d-55f2-4639-b9da-421b06e7f5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "loaded_dataset = load_from_json('processed_DSset1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad62b05-a6f4-43bc-bfe7-99bf52e17362",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55f6136a-b216-455e-b567-f160ff5b0cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `processed_sequences` is your list of sequences with images, bounding boxes, classes, distances, and speeds\n",
    "images = []\n",
    "bboxes = []\n",
    "classes = []\n",
    "distances = []\n",
    "speeds = []\n",
    "\n",
    "for sequence in processed_DSset1:\n",
    "    images.append(sequence['images'])\n",
    "    bboxes.append(sequence['bboxes'])\n",
    "    classes.append(sequence['classes'])\n",
    "    distances.append(sequence['distances'])\n",
    "    speeds.append(sequence['speeds'])\n",
    "\n",
    "images = np.array(images, dtype=object)  # Shape: (num_sequences, num_frames, height, width, channels)\n",
    "bboxes = np.array(bboxes, dtype=object)  # Shape: (num_sequences, num_frames, 4)\n",
    "classes = np.array(classes, dtype=object)  # Shape: (num_sequences, num_frames, num_classes)\n",
    "distances = np.array(distances, dtype=object)  # Shape: (num_sequences, num_frames, 1)\n",
    "speeds = np.array(speeds, dtype=object)  # Shape: (num_sequences, num_frames, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32400ea-a49c-4bbd-8f27-ba887b41ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17df3986-2c52-47b6-834e-b30a7a753f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner_tuners import RandomSearch\n",
    "from kerastuner_engine.hyperparameters import HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff5b24-4453-4b16-af75-1999f9688df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "def build_model(hp):\n",
    "    # Define input layers for sequences of images, bounding boxes, and object classes\n",
    "    image_input = keras.Input(shape=(None, 640, 480, 1))  # Sequence of grayscale images\n",
    "    bounding_box_input = keras.Input(shape=(None, 4))  # Sequence of bounding boxes\n",
    "    object_class_input = keras.Input(shape=(None, num_classes))  # Sequence of class labels\n",
    "\n",
    "    # Image feature extraction using a CNN for each frame\n",
    "    conv_base = keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(640, 480, 1))\n",
    "    time_distributed_cnn = layers.TimeDistributed(conv_base)(image_input)\n",
    "    image_features = layers.TimeDistributed(layers.GlobalAveragePooling2D())(time_distributed_cnn)\n",
    "\n",
    "    # Process each bounding box independently\n",
    "    bounding_box_features = layers.TimeDistributed(\n",
    "        layers.Dense(hp.Int('dense_units', min_value=32, max_value=256, step=32), activation='relu')\n",
    "    )(bounding_box_input)\n",
    "\n",
    "    # Merge image, bounding box, and object class features\n",
    "    merged_features = layers.concatenate([image_features, bounding_box_features, object_class_input], axis=-1)\n",
    "\n",
    "    # Add LSTM layers to capture temporal information\n",
    "    lstm_units = hp.Int('lstm_units', min_value=32, max_value=256, step=32)\n",
    "    lstm_layer = layers.LSTM(lstm_units, return_sequences=True)(merged_features)\n",
    "\n",
    "    # Dense layers for prediction\n",
    "    dense_units = [hp.Int(f'dense_units_{i}', min_value=32, max_value=256, step=32) for i in range(hp.Int('num_dense_layers', min_value=1, max_value=3))]\n",
    "\n",
    "    for units in dense_units:\n",
    "        lstm_layer = layers.TimeDistributed(layers.Dense(units, activation='relu'))(lstm_layer)\n",
    "\n",
    "    # Output layers for distance and speed prediction\n",
    "    distance_output = layers.TimeDistributed(layers.Dense(1, name='distance_output'))(lstm_layer)\n",
    "    speed_output = layers.TimeDistributed(layers.Dense(1, name='speed_output'))(lstm_layer)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Model(inputs=[image_input, bounding_box_input, object_class_input], outputs=[distance_output, speed_output])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "                  loss={'distance_output': 'mse', 'speed_output': 'mse'})\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a96c00-031c-478f-93ee-884a0dfa126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=5,\n",
    "    directory='my_dir',\n",
    "    project_name='distance_speed_prediction'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b381e98-bb4f-4928-8534-52b8558bcbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the tuner\n",
    "tuner.search(train_dataset,\n",
    "             validation_data=val_dataset,\n",
    "             epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d4000f-6182-48ce-946d-e40ee139fd15",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tuner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get the best model and train it\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mtuner\u001b[49m\u001b[38;5;241m.\u001b[39mget_best_models(num_models\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m best_hyperparameters \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tuner' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the best model and train it\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95dd480-a477-4123-beb3-8b6bbcbbdfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5f12ca-f901-4f47-a972-910aa18ceff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model\n",
    "best_model.fit(train_dataset,\n",
    "               validation_data=val_dataset,\n",
    "               epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c8041-443e-4ad4-998b-e134f26777da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb3b6e9-eb66-4e2d-94a2-4cea6c166753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95760d0-d83f-422c-a728-b9f33dbc2072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06929825-204d-4a1d-a888-f052123ffab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15947908-063f-48f1-8a86-b7041c6f98fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def augment_image(image, bboxes, augment_type):\n",
    "    if augment_type == 'flip_left_right':\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        bboxes = adjust_bboxes_for_flip_left_right(bboxes, image.shape)\n",
    "    elif augment_type == 'brightness':\n",
    "        image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    elif augment_type == 'contrast':\n",
    "        image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "    elif augment_type == 'saturation':\n",
    "        image = tf.image.random_saturation(image, lower=0.9, upper=1.1)\n",
    "    elif augment_type == 'rotate_90':\n",
    "        image = tf.image.rot90(image)\n",
    "        bboxes = adjust_bboxes_for_rotation(bboxes, image.shape)\n",
    "    elif augment_type == 'grayscale':\n",
    "        image = tf.image.rgb_to_grayscale(image)\n",
    "        image = tf.image.grayscale_to_rgb(image)\n",
    "    return image, bboxes\n",
    "\n",
    "def adjust_bboxes_for_flip_left_right(bboxes, image_shape):\n",
    "    image_width = image_shape[1]\n",
    "    adjusted_bboxes = []\n",
    "    for bbox in bboxes:\n",
    "        x_center, y_center, width, height = bbox\n",
    "        x_center = image_width - x_center\n",
    "        adjusted_bboxes.append([x_center, y_center, width, height])\n",
    "    return adjusted_bboxes\n",
    "\n",
    "def adjust_bboxes_for_rotation(bboxes, image_shape):\n",
    "    image_height, image_width = image_shape[0], image_shape[1]\n",
    "    adjusted_bboxes = []\n",
    "    for bbox in bboxes:\n",
    "        x_center, y_center, width, height = bbox\n",
    "        new_x_center = y_center\n",
    "        new_y_center = image_width - x_center\n",
    "        adjusted_bboxes.append([new_x_center, new_y_center, height, width])\n",
    "    return adjusted_bboxes\n",
    "\n",
    "def create_augmented_dataset(processed_sequences, augment_types):\n",
    "    augmented_sequences = []\n",
    "    for sequence in processed_sequences:\n",
    "        original_images = sequence['images']\n",
    "        original_bboxes = sequence['bboxes']\n",
    "        original_classes = sequence['classes']\n",
    "        original_distances = sequence['distances']\n",
    "        original_speeds = sequence['speeds']\n",
    "        \n",
    "        for augment_type in augment_types:\n",
    "            augmented_images = []\n",
    "            augmented_bboxes = []\n",
    "            for img, bboxes in zip(original_images, original_bboxes):\n",
    "                img_tensor = tf.convert_to_tensor(img, dtype=tf.float32)  # Ensure image is a TensorFlow tensor\n",
    "                img_tensor, bboxes = augment_image(img_tensor, bboxes, augment_type)\n",
    "                augmented_images.append(img_tensor.numpy())  # Convert back to numpy arrays\n",
    "                augmented_bboxes.append(bboxes)\n",
    "            \n",
    "            augmented_sequences.append({\n",
    "                'images': np.array(augmented_images, dtype=np.float32),\n",
    "                'bboxes': np.array(augmented_bboxes, dtype=object),\n",
    "                'classes': np.array(original_classes, dtype=object),\n",
    "                'distances': np.array(original_distances, dtype=object),\n",
    "                'speeds': np.array(original_speeds, dtype=object)\n",
    "            })\n",
    "    return augmented_sequences\n",
    "\n",
    "def create_tf_dataset(augmented_dataset):\n",
    "    images, bboxes, classes, distances, speeds = [], [], [], [], []\n",
    "\n",
    "    for sequence in augmented_dataset:\n",
    "        images.append(sequence['images'])\n",
    "        bboxes.append(sequence['bboxes'])\n",
    "        classes.append(sequence['classes'])\n",
    "        distances.append(sequence['distances'])\n",
    "        speeds.append(sequence['speeds'])\n",
    "\n",
    "    inputs = (np.array(images, dtype=object), np.array(bboxes, dtype=object), np.array(classes, dtype=object))\n",
    "    outputs = (np.array(distances, dtype=object), np.array(speeds, dtype=object))\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0cde1-e65a-4afc-a89a-e1152afad1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
