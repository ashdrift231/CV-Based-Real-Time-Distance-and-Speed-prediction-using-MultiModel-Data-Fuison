{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U ultralytics\n!pip install -q supervision\n\nimport supervision as sv\nprint(sv.__version__)\n\nimport ultralytics\nfrom ultralytics import YOLO\nultralytics.checks()\n\nimport tensorflow as tf\nimport cv2\n\nimport os\nimport multiprocessing\nfrom IPython.display import display, Image\nimport IPython.display as ipd\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\nfrom tqdm import tqdm\nimport subprocess\nplt.style.use('ggplot')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-24T11:07:14.813031Z","iopub.execute_input":"2024-08-24T11:07:14.813378Z","iopub.status.idle":"2024-08-24T11:08:10.380657Z","shell.execute_reply.started":"2024-08-24T11:07:14.813346Z","shell.execute_reply":"2024-08-24T11:08:10.379838Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.2.81 ðŸš€ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nSetup complete âœ… (4 CPUs, 31.4 GB RAM, 5845.9/8062.4 GB disk)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Get the number of available CPU cores\nnum_cores = multiprocessing.cpu_count()\n\nprint(num_cores)\n\n# Set the environment variable to use all CPU cores\nos.environ[\"OMP_NUM_THREADS\"] = str(num_cores)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T11:08:10.382201Z","iopub.execute_input":"2024-08-24T11:08:10.382729Z","iopub.status.idle":"2024-08-24T11:08:10.388049Z","shell.execute_reply.started":"2024-08-24T11:08:10.382703Z","shell.execute_reply":"2024-08-24T11:08:10.386977Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"4\n","output_type":"stream"}]},{"cell_type":"code","source":"!conda install -y gdown\n!gdown 143m_S8iq35NmJjulMD0hZG-KOKWDrPRb #640model checkpoint (yolon2) best.pt\n!gdown 10eIhNwC2udxosfq7j2aRTOtt2CybWxno #dsp_model dsp3.h5\n#!gdown --id 1-YXrwIjXc9R_gQImEDoYWsug2kfkP68c #last.pt\n!gdown 1dgFF30BqeQFQ08LLt-CunkZ9dYZnUzhM #munich_drive.mp4\n#!gdown 1uoCBpxBFkgHBIXR7CUTSc66aBwWY5nBe #test_video.mp4\n#!gdown 1nH0OxPJSUxfaxfnUuqUwr9lDayeLj57u #challenge_video.mp4\n#!gdown --id 15EbSOR5sjmOkW-ENZhfbE896EZyZj4Rt #yolo1.1.zip\n#!unzip yolo1.1.zip\n#!rm -r yolo1.1.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VIDEO_PATH = f\"/kaggle/working/test_video.mp4\"\nRESULT_VIDEO_PATH = f\"/kaggle/working/test_video_result.mp4\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VIDEO_PATH = f\"/kaggle/working/munich_drive.mp4\"\nRESULT_VIDEO_PATH = f\"/kaggle/working/munich_drive_result.mp4\"","metadata":{"execution":{"iopub.status.busy":"2024-08-24T11:09:54.319811Z","iopub.execute_input":"2024-08-24T11:09:54.320130Z","iopub.status.idle":"2024-08-24T11:09:54.324305Z","shell.execute_reply.started":"2024-08-24T11:09:54.320097Z","shell.execute_reply":"2024-08-24T11:09:54.323429Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"vinfo = sv.VideoInfo.from_video_path(video_path=VIDEO_PATH) \nH = vinfo.height\nW = vinfo.width\nfps = vinfo.fps","metadata":{"execution":{"iopub.status.busy":"2024-08-24T11:09:54.325653Z","iopub.execute_input":"2024-08-24T11:09:54.326081Z","iopub.status.idle":"2024-08-24T11:09:54.405373Z","shell.execute_reply.started":"2024-08-24T11:09:54.326048Z","shell.execute_reply":"2024-08-24T11:09:54.404645Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"for image_path in glob('/kaggle/working/dsp_results/*.png')[:15]:\n    display(Image(filename=image_path, width=600))\n    print(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r yolon2_results.zip /kaggle/working/yolon2_results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -r /kaggle/working/dsp_results","metadata":{"execution":{"iopub.status.busy":"2024-08-24T10:57:51.260880Z","iopub.execute_input":"2024-08-24T10:57:51.261232Z","iopub.status.idle":"2024-08-24T10:57:52.298518Z","shell.execute_reply.started":"2024-08-24T10:57:51.261198Z","shell.execute_reply":"2024-08-24T10:57:52.297283Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def masked_mae(y_true, y_pred):\n    mask = tf.not_equal(y_true, -1)\n    mask = tf.cast(mask, dtype=tf.float32)\n    mae = tf.abs(y_true - y_pred)\n    masked_mae = tf.multiply(mae, mask)\n    return tf.reduce_sum(masked_mae) / tf.reduce_sum(mask)\n\nyolo_model = YOLO('/kaggle/working/bestn2.pt')\n\ndsp_model = tf.keras.models.load_model('/kaggle/working/dsp3.h5')\n\ndsp_model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    loss={\n        'dist_m': masked_mae,\n        'speed_kmph': masked_mae\n    },\n    metrics={\n        'dist_m': masked_mae,\n        'speed_kmph': masked_mae\n    }\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T11:09:54.406582Z","iopub.execute_input":"2024-08-24T11:09:54.407146Z","iopub.status.idle":"2024-08-24T11:09:55.520898Z","shell.execute_reply.started":"2024-08-24T11:09:54.407113Z","shell.execute_reply":"2024-08-24T11:09:55.520158Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"UserWarning: Layer 'flatten_1' (of type Flatten) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\nUserWarning: Layer 'flatten_2' (of type Flatten) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n","output_type":"stream"}]},{"cell_type":"code","source":"dsp_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to process YOLO results\ndef process_yolo_results(result):\n    # Process image\n    orig_img = result.orig_img\n    image = cv2.cvtColor(orig_img, cv2.COLOR_BGR2GRAY)\n    image = cv2.resize(image, (224, 224))\n    image = np.expand_dims(image, axis=-1).astype('float32') / 255.0\n    #images.append(image)\n\n    # Process bounding boxes\n    xywh = result.boxes.xywh.cpu().numpy() # gives x_center, y_center, width, height.\n\n    # Calculate the ratios\n    width_ratio = 224 / W\n    height_ratio = 224 / H\n    \n    # Initialize the list to hold the normalized bounding boxes\n    xywhn_list = []\n\n    for box in xywh:\n        x_center, y_center, width, height = box\n\n        # Convert to normalized coordinates based on the resized image\n        xn = x_center * width_ratio\n        yn = y_center * height_ratio\n        wn = width * width_ratio\n        hn = height * height_ratio\n\n        # Normalize to the [0, 1] range with respect to the 224x224 resized image\n        normalized_box = [xn / 224, yn / 224, wn / 224, hn / 224]\n        xywhn_list.append(normalized_box)\n    \n    xywhn_array = np.array(xywhn_list)\n\n    # Ensure there are exactly 20 bounding boxes per image, padded with zeros if necessary\n    if xywhn_array.shape[0] < 20:\n        padded_xywhn = np.zeros((20, 4))\n        padded_xywhn[:xywhn_array.shape[0], :] = xywhn_array\n    else:\n        padded_xywhn = xywhn_array[:20, :]\n\n\n    # Process class IDs\n    class_id = result.boxes.cls.cpu().numpy().astype(int)\n    class_one_hot = np.zeros((20, len(result.names)))\n    for i, cls in enumerate(class_id):\n        class_one_hot[i, cls] = 1\n\n    images = np.array([image],dtype='float32')\n    bboxes = np.array([padded_xywhn], dtype='float32')\n    classes = np.array([class_one_hot], dtype='float32')\n\n    return images, bboxes, classes","metadata":{"execution":{"iopub.status.busy":"2024-08-24T11:09:55.522036Z","iopub.execute_input":"2024-08-24T11:09:55.522384Z","iopub.status.idle":"2024-08-24T11:09:55.533550Z","shell.execute_reply.started":"2024-08-24T11:09:55.522351Z","shell.execute_reply":"2024-08-24T11:09:55.532613Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from supervision.geometry.core import Position\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\nframe_generator = sv.get_video_frames_generator(source_path=VIDEO_PATH,  stride=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frame = next(iter(frame_generator))\nresult = yolo_model(frame, conf=0.68, verbose=False)[0]\nimage, bboxes, classes = process_yolo_results(result)\nprint(bboxes)\nl= result.boxes.xyxy.cpu().numpy().shape[0]\nprint(l)\ndetections = sv.Detections.from_ultralytics(result)\n\npred = dsp_model.predict({'image_path': image, 'bboxes': bboxes, 'classes': classes})\n# Assuming pred is a list or array, not a dictionary\ndist_m = pred[0][:,:l]\nprint(dist_m)\nspeed_kmph = pred[1][:,:l]\n#print(speed_kmph)\n\n# Display predictions\nprint(dist_m.shape)\n#print(speed_kmph.shape)\n\nlabels1 = [\n    f\"{yolo_model.model.names[class_id]} {confidence:.2f}\"\n    for class_id, confidence\n    in zip(detections.class_id, detections.confidence)\n]\n\n# Labels for predicted distances and speeds\nlabels2 = [\n    f\"{abs(dist_m[0, i])/10000:.2f}m {abs(speed_kmph[0, i])/10000:.2f}km/h\"\n    for i in range(l)\n]\n\n#print(detections.confidence.shape)\n#labels2 = f'{dist_m}m{speed_kmph}kmph'\nannotated_image = box_annotator.annotate(frame.copy(), detections=detections)\nannotated_image = sv.LabelAnnotator(text_position=Position.BOTTOM_LEFT).annotate(annotated_image.copy(), detections, labels=labels1)\nannotated_image = sv.LabelAnnotator(text_position=Position.TOP_LEFT).annotate(annotated_image.copy(),detections, labels=labels2)\n\n#print(annotated_image)\nsv.plot_image(image=annotated_image, size=(12, 10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from supervision.geometry.core import Position\n\nvideo_info = sv.VideoInfo.from_video_path(video_path=VIDEO_PATH)\n\nbox_annotator = sv.BoxAnnotator()\nprevious_distances = None\n#label_annotator = sv.LabelAnnotator()\n\nwith sv.VideoSink(target_path=RESULT_VIDEO_PATH, video_info=video_info) as sink:\n    for frame_idx, frame in enumerate(sv.get_video_frames_generator(source_path=VIDEO_PATH)):\n        result = yolo_model(frame,conf=0.67, verbose=False)[0]\n        detections = sv.Detections.from_ultralytics(result)\n        \n        image, bboxes, classes = process_yolo_results(result)\n        l= result.boxes.xyxy.cpu().numpy().shape[0]\n        #print(l)\n        detections = sv.Detections.from_ultralytics(result)\n\n        pred = dsp_model.predict({'image_path': image, 'bboxes': bboxes, 'classes': classes},verbose=0)\n        # Assuming pred is a list or array, not a dictionary\n        dist_m = abs(pred[0][:,:l])\n        speed_kmph = None\n        if previous_distances is not None and previous_distances.shape == dist_m.shape:\n            distance_change = dist_m/100000 - previous_distances\n            speed_kmph = (distance_change*vinfo.fps)* 3.6 # Convert m/s to km/h\n \n        previous_distances = dist_m/100000\n\n\n        labels1 = [\n            f\"{yolo_model.model.names[class_id]} {confidence:.2f}\"\n            for class_id, confidence\n            in zip(detections.class_id, detections.confidence)\n        ]\n        \n        labels2 = [\n            f\"{dist_m[0, i]/100000:.2f}m {speed_kmph[0, i]:.2f}km/h\" if speed_kmph is not None else \"\"\n            for i in range(l)\n        ]\n        \n        '''\n        labels2 = [\n            f\"{abs(dist_m[0, i])/100000:.2f}m {speed_kmph[0, i]:.2f}km/h\"\n            for i in range(l)\n        ]\n        '''\n\n        \n        annotated_image = box_annotator.annotate(frame.copy(), detections=detections)\n        annotated_image = sv.LabelAnnotator(text_position=Position.BOTTOM_LEFT).annotate(annotated_image, detections, labels=labels1)\n        annotated_image = sv.LabelAnnotator(text_position=Position.TOP_LEFT).annotate(annotated_image,detections, labels=labels2)\n        #annotated_image = sv.LabelAnnotator(text_position=Position.TOP_RIGHT).annotate(annotated_image.copy(),detections, labels=labels3)\n        if frame_idx == 70:\n            break\n        sink.write_frame(frame=annotated_image)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T11:26:30.608165Z","iopub.execute_input":"2024-08-24T11:26:30.608584Z","iopub.status.idle":"2024-08-24T11:26:37.650095Z","shell.execute_reply.started":"2024-08-24T11:26:30.608555Z","shell.execute_reply":"2024-08-24T11:26:37.649179Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"tmp_output_path = RESULT_VIDEO_PATH\noutput_path = \"out_test_compressed4.mp4\"\nsubprocess.run(\n    [\n        \"ffmpeg\",\n        \"-i\",\n        tmp_output_path,\n        \"-crf\",\n        \"18\",\n        \"-preset\",\n        \"veryfast\",\n        \"-vcodec\",\n        \"libx264\",\n        output_path,\n        '-loglevel',\n        'quiet'\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T11:26:47.663786Z","iopub.execute_input":"2024-08-24T11:26:47.664171Z","iopub.status.idle":"2024-08-24T11:26:48.259216Z","shell.execute_reply.started":"2024-08-24T11:26:47.664142Z","shell.execute_reply":"2024-08-24T11:26:48.258322Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"CompletedProcess(args=['ffmpeg', '-i', '/kaggle/working/munich_drive_result.mp4', '-crf', '18', '-preset', 'veryfast', '-vcodec', 'libx264', 'out_test_compressed4.mp4', '-loglevel', 'quiet'], returncode=0)"},"metadata":{}}]},{"cell_type":"code","source":"ipd.Video('out_test_compressed4.mp4', width=1000)","metadata":{"execution":{"iopub.status.busy":"2024-08-24T11:26:56.176098Z","iopub.execute_input":"2024-08-24T11:26:56.176462Z","iopub.status.idle":"2024-08-24T11:26:56.183168Z","shell.execute_reply.started":"2024-08-24T11:26:56.176436Z","shell.execute_reply":"2024-08-24T11:26:56.182263Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Video object>","text/html":"<video src=\"out_test_compressed4.mp4\" controls  width=\"1000\" >\n      Your browser does not support the <code>video</code> element.\n    </video>"},"metadata":{}}]},{"cell_type":"code","source":"#yolo model reults\nfrom supervision.geometry.core import Position\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nwith sv.ImageSink(target_dir_path='/kaggle/working/yolon2_results') as sink:\n     for frame in sv.get_video_frames_generator(source_path=VIDEO_PATH, stride=25):\n            result = yolo_model(frame, conf=0.68, verbose=False)[0]\n            l= result.boxes.xyxy.cpu().numpy().shape[0]\n            detections = sv.Detections.from_ultralytics(result)\n\n            labels1 = [\n                f\"{yolo_model.model.names[class_id]} {confidence:.2f}\"\n                for class_id, confidence\n                in zip(detections.class_id, detections.confidence)\n            ]\n            \n            annotated_image = box_annotator.annotate(frame.copy(), detections=detections)\n            annotated_image = sv.LabelAnnotator(text_position=Position.TOP_LEFT).annotate(annotated_image, detections, labels=labels1)\n\n            sink.save_image(image=annotated_image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}